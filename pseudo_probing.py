# -*- coding: utf-8 -*-
"""pseudo-probing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DPHUdk-SZLIEITKVfkozT5JySdd1XqyT

# imports
"""

# import torch.nn as nn
# import torch.nn.functional as F
# import torch
from collections import OrderedDict
# import torch
# from sklearn.metrics import f1_score
# from torch.nn.functional import cross_entropy
from torch.utils.data import Dataset
# import pandas as pd
import h5py
import json
import logging
import os
# !pip install py7zr > /dev/null
import torch
from sklearn.metrics import f1_score
import torch.nn as nn
from torch.nn.functional import cross_entropy
# from torch.optim.lr_scheduler import LinearLR
import pandas as pd
from tqdm import tqdm
import numpy

import requests
import os
import zipfile
# import py7zr
import csv

logging.basicConfig(
    level=logging.INFO,  # Set the minimum log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    format='%(asctime)s - %(name)s.%(lineno)d - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),  # Log to console
        logging.FileHandler("results/log.txt", mode='w'),
    ]
)
logger = logging.getLogger(__name__)
"""# contact f1"""

def contact_f1(ref_batch, pred_batch, Ls, th=0.5, reduce=True, method="triangular"):
    """Compute F1 from base pairs. Input goes to sigmoid and then thresholded"""
    f1_list = []

    if type(ref_batch) == float or len(ref_batch.shape) < 3:
        ref_batch = [ref_batch]
        pred_batch = [pred_batch]
        L = [L]

    for ref, pred, l in zip(ref_batch, pred_batch, Ls):
        # ignore padding
        ind = torch.where(ref != -1)
        pred = pred[ind].view(l, l)
        ref = ref[ind].view(l, l)

        # pred goes from -inf to inf
        pred = torch.sigmoid(pred)
        pred[pred<=th] = 0

        if method == "triangular":
            f1 = f1_triangular(ref, pred>th)
        elif method == "shift":
            raise NotImplementedError
            # ref = mat2bp(ref)
            # pred = mat2bp(pred)
            # _, _, f1 = f1_shift(ref, pred)
        else:
            raise NotImplementedError

        f1_list.append(f1)

    if reduce:
        return torch.tensor(f1_list).mean().item()
    else:
        return torch.tensor(f1_list)


def f1_triangular(ref, pred):
    """Compute F1 from the upper triangular connection matrix"""
    # get upper triangular matrix without diagonal
    ind = torch.triu_indices(ref.shape[0], ref.shape[1], offset=1)

    ref = ref[ind[0], ind[1]].numpy().ravel()
    pred = pred[ind[0], ind[1]].numpy().ravel()

    return f1_score(ref, pred, zero_division=0)

"""# RiNALMo secondary structure model"""

def get_embed_dim(loader):
    # grab an element from the loader, which is represented by a dictionary with keys
    # `seq_ids`, `seq_embs_pad`, `contacts`, `Ls`
    batch_elem = next(iter(loader))
    # query for `seq_embs_pad` key (containing the embedding representations of all the sequences in the batch)
    # whose size will be batch_size x L x d
    return batch_elem["seq_embs_pad"].shape[2]

def outer_concat(t1: torch.Tensor, t2: torch.Tensor):
    # t1, t2: shape = B x L x E
    assert t1.shape == t2.shape, f"Shapes of input tensors must match! ({t1.shape} != {t2.shape})"

    seq_len = t1.shape[1]
    a = t1.unsqueeze(-2).expand(-1, -1, seq_len, -1)
    b = t2.unsqueeze(-3).expand(-1, seq_len, -1, -1)

    return torch.concat((a, b), dim=-1)

def mat2bp(x):
    """Get base-pairs from conection matrix [N, N]. It uses upper
    triangular matrix only, without the diagonal. Positions are 1-based. """
    ind = torch.triu_indices(x.shape[0], x.shape[1], offset=1)
    pairs_ind = torch.where(x[ind[0], ind[1]] > 0)[0]

    pairs_ind = ind[:, pairs_ind].T
    # remove multiplets pairs
    multiplets = []
    for i, j in pairs_ind:
        ind = torch.where(pairs_ind[:, 1]==i)[0]
        if len(ind)>0:
            pairs = [bp.tolist() for bp in pairs_ind[ind]] + [[i.item(), j.item()]]
            best_pair = torch.tensor([x[bp[0], bp[1]] for bp in pairs]).argmax()

            multiplets += [pairs[k] for k in range(len(pairs)) if k!=best_pair]

    pairs_ind = [[bp[0]+1, bp[1]+1] for bp in pairs_ind.tolist() if bp not in multiplets]

    return pairs_ind

def contact_f1(ref_batch, pred_batch, Ls, th=0.5, reduce=True, method="triangular"):
    """Compute F1 from base pairs. Input goes to sigmoid and then thresholded"""
    f1_list = []

    if type(ref_batch) == float or len(ref_batch.shape) < 3:
        ref_batch = [ref_batch]
        pred_batch = [pred_batch]
        L = [L]

    for ref, pred, l in zip(ref_batch, pred_batch, Ls):
        # ignore padding
        ind = torch.where(ref != -1)
        pred = pred[ind].view(l, l)
        ref = ref[ind].view(l, l)

        # pred goes from -inf to inf
        pred = torch.sigmoid(pred)
        pred[pred<=th] = 0

        if method == "triangular":
            f1 = f1_triangular(ref, pred>0)
        if method == "f1_shift":
            ref_bp = mat2bp(ref)
            pred_bp = mat2bp(pred)
            f1 = f1_shift(ref_bp, pred_bp)

        f1_list.append(f1)

    if reduce:
        return torch.tensor(f1_list).mean().item()
    else:
        return torch.tensor(f1_list)

def f1_triangular(ref, pred):
    """Compute F1 from the upper triangular connection matrix"""
    # get upper triangular matrix without diagonal
    ind = torch.triu_indices(ref.shape[0], ref.shape[1], offset=1)

    ref = ref[ind[0], ind[1]].numpy().ravel()
    pred = pred[ind[0], ind[1]].numpy().ravel()

    return f1_score(ref, pred, zero_division=0)


class ResNet2DBlock(nn.Module):
    def __init__(self, embed_dim, kernel_size=3, bias=False):
        super().__init__()

        # Bottleneck architecture
        self.conv_net = nn.Sequential(
            nn.Conv2d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=1, bias=bias),
            nn.InstanceNorm2d(embed_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=kernel_size, bias=bias, padding="same"),
            nn.InstanceNorm2d(embed_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels=embed_dim, out_channels=embed_dim, kernel_size=1, bias=bias),
            nn.InstanceNorm2d(embed_dim),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        residual = x

        x = self.conv_net(x)
        x = x + residual

        return x

class ResNet2D(nn.Module):
    def __init__(self, embed_dim, num_blocks, kernel_size=3, bias=False):
        super().__init__()

        self.blocks = nn.ModuleList(
            [
                ResNet2DBlock(embed_dim, kernel_size, bias=bias) for _ in range(num_blocks)
            ]
        )

    def forward(self, x):
        for block in self.blocks:
            x = block(x)

        return x

class SecondaryStructurePredictor(nn.Module):
    def __init__(
        self, embed_dim, num_blocks=2,
        conv_dim=64, kernel_size=3,
        negative_weight=0.1,
        device='cpu', lr=1e-5
    ):
        super().__init__()
        self.lr = lr
        self.threshold = 0.1
        self.linear_in = nn.Linear(embed_dim, (int) (conv_dim/2))
        self.resnet = ResNet2D(conv_dim, num_blocks, kernel_size)
        self.conv_out = nn.Conv2d(conv_dim, 1, kernel_size=kernel_size, padding="same")
        self.device = device
        self.class_weight = torch.tensor([negative_weight, 1.0]).float().to(self.device)
        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
        # self.lr_scheduler = LinearLR(self.optimizer, start_factor=1.0, end_factor=0.1, total_iters=2000)

        self.to(device)

    def loss_func(self, yhat, y):
        """yhat and y are [N, M, M]"""
        y = y.view(y.shape[0], -1)

        yhat = yhat.view(yhat.shape[0], -1)

        yhat = yhat.unsqueeze(1)
        yhat = torch.cat((-yhat, yhat), dim=1)

        loss = cross_entropy(yhat, y, ignore_index=-1, weight=self.class_weight)

        return loss

    def forward(self, x):
        x = self.linear_in(x)

        x = outer_concat(x, x)
        x = x.permute(0, 3, 1, 2)

        x = self.resnet(x)
        x = self.conv_out(x)
        x = x.squeeze(-3)

        x = torch.triu(x, diagonal=1)
        x = x + x.transpose(-1, -2)

        return x.squeeze(-1)

    def fit(self, loader):
        self.train()
        loss_acum = 0
        f1_acum = 0
        for batch in tqdm(loader):
            X = batch["seq_embs_pad"].to(self.device)
            y = batch["contacts"].to(self.device)
            y_pred = self(X)
            # print(f"y_pred size: {y_pred.shape}") # torch.Size([4, 512, 512])
            # print(f"y size: {y.shape}") # torch.Size([4, 512, 512])
            loss = self.loss_func(y_pred, y)
            loss_acum += loss.item()
            f1_acum += contact_f1(y.cpu(), y_pred.detach().cpu(), batch["Ls"], method="triangular")
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
        # self.lr_scheduler.step()
        loss_acum /= len(loader)
        f1_acum /= len(loader)
        return {"loss": loss_acum, "f1": f1_acum}

    def test(self, loader):
        self.eval()
        loss_acum = 0
        f1_acum = 0
        for batch in loader:
            X = batch["seq_embs_pad"].to(self.device)
            y = batch["contacts"].to(self.device)
            with torch.no_grad():
                y_pred = self(X)
                loss = self.loss_func(y_pred, y)
            loss_acum += loss.item()

            f1_acum += contact_f1(y.cpu(), y_pred.detach().cpu(), batch["Ls"], method="triangular")
        loss_acum /= len(loader)
        f1_acum /= len(loader)

        return {"loss": loss_acum, "f1": f1_acum}

    def pred(self, loader):
        self.eval()

        predictions = []
        for batch in loader:

            Ls = batch["Ls"]
            seq_ids = batch["seq_ids"]
            sequences = batch["sequences"]
            X = batch["seq_embs_pad"].to(self.device)
            with torch.no_grad():
                y_pred = self(X)

            for k in range(len(y_pred)):
                predictions.append((
                    seq_ids[k],
                    sequences[k],
                    mat2bp(
                        y_pred[k, : Ls[k], : Ls[k]].squeeze().cpu()
                    )
                ))
        predictions = pd.DataFrame(predictions, columns=["id", "sequence", "base_pairs"])

        return predictions

"""# Dataset related code"""

import torch
from torch.utils.data import Dataset
import pandas as pd
import h5py
import json

def bp2matrix(L, base_pairs):
    matrix = torch.zeros((L, L))
    # base pairs are 1-based
    bp = torch.tensor(base_pairs) - 1
    if len(bp.shape) == 2:
        matrix[bp[:, 0], bp[:, 1]] = 1
        matrix[bp[:, 1], bp[:, 0]] = 1

    return matrix

class EmbeddingDataset(Dataset):
    def __init__(
        self, dataset_path, embedding_name, probing_path, beta):

        # Loading dataset
        data = pd.read_csv(dataset_path)
        self.sequences = data.sequence.tolist()
        self.ids = data.id.tolist()

        # Loading representations
        self.embeddings = {}
        try:
            embeddings = h5py.File(f"data/embeddings/{embedding_name}_ArchiveII.h5", "r")
        except FileNotFoundError:
            print(f"Embedding file not found: {embedding_name}")
            raise

        # Loading probing
        try:
            probing = torch.load(probing_path)
        except FileNotFoundError:
            print(f"Probing file not found: {probing_path}")
            raise
        # keep only sequences in dataset_path
        for seq_id in self.ids:
          embedding = torch.from_numpy(embeddings[seq_id][()])
          probing_reshaped = probing[seq_id].reshape(probing[seq_id].shape[0], 1)
          probing_reshaped = (1-beta)*probing_reshaped + beta*numpy.random.uniform(0, 1, probing_reshaped.shape)
          embedding = torch.from_numpy(numpy.hstack([embedding, probing_reshaped])) # L x d - > L x d+1
          self.embeddings[seq_id] = embedding

        self.base_pairs = None
        self.base_pairs = [
            json.loads(data.base_pairs.iloc[i]) for i in range(len(data))
        ]
    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        seq_id = self.ids[idx]
        sequence = self.sequences[idx]

        if seq_id not in self.embeddings:
            print(f"{seq_id} not present")
            raise
        seq_emb = self.embeddings[seq_id]
        L = len(sequence)

        Mc = bp2matrix(L, self.base_pairs[idx])
        return {"seq_id": seq_id, "seq_emb": seq_emb, "contact": Mc, "L": L, "sequence": sequence}

def pad_batch(batch):
    """batch is a list of dicts with keys: seqid, seq_emb, Mc, L, sequence, mask"""
    seq_ids, seq_embs, Mcs, Ls, sequences = [[batch_elem[key] for batch_elem in batch] for key in batch[0].keys()]
    # should embedding_dim be computed for every batch?
    embedding_dim = seq_embs[0].shape[1] # seq_embs is a list of tensors of size L x d
    batch_size = len(batch)
    max_L = max(Ls)
    seq_embs_pad = torch.zeros(batch_size, max_L, embedding_dim)
    # cross entropy loss can ignore the -1s
    Mcs_pad = -torch.ones((batch_size, max_L, max_L), dtype=torch.long)
    for k in range(batch_size):
        seq_embs_pad[k, : Ls[k], :] = seq_embs[k][:Ls[k], :]
        Mcs_pad[k, : Ls[k], : Ls[k]] = Mcs[k]
    return {"seq_ids": seq_ids, "seq_embs_pad": seq_embs_pad, "contacts": Mcs_pad, "Ls": Ls, "sequences": sequences}


def create_dataloader(embedding_name, partition_path, probing_path, batch_size, shuffle, beta, collate_fn=pad_batch):
    dataset = EmbeddingDataset(
        embedding_name=embedding_name,
        dataset_path=partition_path,
        probing_path=probing_path,
        beta=beta,
    )
    return torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        collate_fn=collate_fn,
    )

"""# training code"""


batch_size = 4
max_epochs = 500

if torch.cuda.is_available():
    device=f"cuda:{torch.cuda.current_device()}"
else:
    device='cpu'
lr=1e-4

import shutil

df = pd.read_csv(f'data/ArchiveII.csv', index_col="id")
splits = pd.read_csv(f'data/ArchiveII_famfold_splits.csv', index_col="id")

for fam in splits.fold.unique():
    if fam!='5s':
        print(f"skipping {fam}")
        continue
    train = df.loc[splits[(splits.fold==fam) & (splits.partition!="test")].index]
    test = df.loc[splits[(splits.fold==fam) & (splits.partition=="test")].index]
    data_path = f"data/{fam}"
    out_path = f"results/{fam}"
    os.makedirs(data_path, exist_ok=True)
    # shutil.rmtree(out_path, ignore_errors=True)
    os.makedirs(out_path, exist_ok=True)
    train.to_csv(f"{data_path}/train.csv")
    test.to_csv(f"{data_path}/test.csv")
    logger.info("+" * 80)
    logger.info(f"ArchiveII {fam} TRAINING STARTED".center(80))
    logger.info("+" * 80)

    # embed_dim = get_embed_dim(train_loader) # hardcode embedding dimension
    net = SecondaryStructurePredictor(embed_dim=5, device=device, lr=lr)

    metrics_for_epoch = []
    # logger.info(f"Run on {args.out_path}, with device {args.device} and embeddings {args.embeddings_path}")
    # logger.info(f"Training with file: {args.train_partition_path}")
    noise_added = False # flag to indicate if noise was increased in the current epoch
    add_noise = False # flag to indicate whether to add noise or not (False for the first couple of epochs, True and not changed after such epochs)
    previous_loss = float('inf') # init previous loss to something
    best_loss_dict = [{"epoch": -1, "loss": float('inf')}] # this was only a numeric value before, added a dict for debugging purposes
    t=3 # initial noise step
    T=10 # max noise steps
    tolerance=1e-5 # tolerance to interpret two consecutive loss values as equal
    perc=0.001 # percentaje that best/current loss ratio must reach for noise to be added
    logger.info(f"noise steps: {T}")
    logger.info(f"tol: {tolerance}")
    logger.info(f"max epochs: {max_epochs}")
    # metrics={
    #     "train_loss":
    # }
    csv_path = os.path.join(out_path, "metrics.csv")
    fieldnames = [
        "train_loss", "train_f1",
        "val_loss", "val_f1", 
        "noise_added", "beta",
        "epoch"
    ]
    with open(csv_path, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
    for epoch in range(max_epochs):
        logger.info(f"starting epoch {epoch}")

        beta = t/T
        if add_noise:
            if beta>1:
                beta=1
            logger.info(f"adding {beta} noise")
        else:
            # workaround to not add noise in the very first epochs
            logger.info("not adding noise")
            beta=0
        logger.info(f"current noise step: {t}")
        logger.info(f"max noise steps: {T}")
        logger.info(f"beta: {beta}")

        train_loader = create_dataloader(
            "one-hot",
            f"{data_path}/train.csv",
            "data/ArchiveII_probing.pt",
            batch_size,
            True,
            beta=beta,
        )
        metrics = net.fit(train_loader)

        metrics = {f"train_{k}": v for k, v in metrics.items()}

        val_loader = create_dataloader(
            "one-hot",
            f"{data_path}/test.csv",
            "data/ArchiveII_probing.pt",
            batch_size,
            False,
            beta=beta,
        )
        logger.info("running inference")
        val_metrics = net.test(val_loader)

        val_metrics = {f"val_{k}": v for k, v in val_metrics.items()}
        metrics.update(val_metrics)

        noise_metrics={"noise_added": noise_added, "beta": beta}
        metrics.update(noise_metrics)

        current_loss = metrics['train_loss']
        best_loss = best_loss_dict[-1].get('loss')
        closeness_perc = (current_loss-best_loss)/best_loss
        close_to_best = closeness_perc < perc
        print(f"closeness perc is: {closeness_perc}")
        print(f"close to best is: {close_to_best} ")
        if current_loss - previous_loss > tolerance: # positive difference between losses
            logger.info("loss worsened, not adding noise")
            noise_added = False
        elif (not add_noise and abs(current_loss - previous_loss) <= tolerance) or (add_noise and close_to_best):
            # add noise
            logger.info(f"loss reached plateau, or we are close to best, adding noise")
            noise_added = True
            add_noise = True
            t+=.1

            logger.info("Resetting optimizer state")
            net.optimizer = torch.optim.Adam(net.parameters(), lr=lr)

            logger.info("updating best loss")
            best_loss_dict.append({"epoch": epoch, "loss": current_loss})
            logger.info(f"last entry best loss dict: {best_loss_dict[-1]}")
        else:
            logger.info("loss improved, not adding noise")
            noise_added = False
            if not add_noise: # just update best loss "instantaneously" for the very first epochs
                logger.info("updating best loss")
                best_loss_dict.append({"epoch": epoch, "loss": current_loss})

        previous_loss = current_loss

        # metrics_for_epoch.append(metrics)
        with open(csv_path, 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=metrics.keys())
            writer.writerow(metrics)
        logger.info(" ".join([f"{k}: {v}" for k, v in metrics.items()]))

    # os.system(f"python src/train_model.py --emb {args.emb} --train_partition_path {data_path}train.csv --out_path {out_path}")
    # torch.save(
    #   net.state_dict(),
    #   f"results/pseudo-probing/{fam}/weights.pmt",
    # )

    logger.info(f"ArchiveII {fam} TRAINING ENDED".center(80))
    # pd.set_option('display.float_format','{:.3f}'.format)
    # pd.DataFrame(metrics_for_epoch).to_csv(os.path.join(out_path, f"metrics.csv"), index=False)
    # logger.info("+" * 80)
    # logger.info(f"ArchiveII {fam} TESTING STARTED".center(80))
    # logger.info("+" * 80)

    # test_loader = create_dataloader(
    #     "one-hot",
    #     f"{data_path}/test.csv",
    #     "data/ArchiveII_probing.pt",
    #     batch_size,
    #     False
    # )

    # # embed_dim = get_embed_dim(test_loader)
    # best_model = SecondaryStructurePredictor(embed_dim=embed_dim, device=device)
    # best_model.load_state_dict(torch.load(f"results/pseudo-probing/{fam}/weights.pmt", map_location=torch.device(best_model.device)))
    # best_model.eval()


    # metrics = best_model.test(test_loader)
    # metrics = {f"test_{k}": v for k, v in metrics.items()}
    # logger.info(" ".join([f"{k}: {v:.3f}" for k, v in metrics.items()]))
    # logger.info(f"ArchiveII {fam} TESTING ENDED".center(80))
    # break
    # # os.system(f"python src/test_model.py --emb {args.emb} --test_partition_path {data_path}test.csv --out_path {out_path}")
    # # print(f"ArchiveII {fam} TESTING ENDED".center(80))
